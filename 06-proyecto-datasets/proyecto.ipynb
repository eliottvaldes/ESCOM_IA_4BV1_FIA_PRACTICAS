{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESCOM - IIA\n",
    "#### FUNDAMENTOS DE INTELIGENCIA ARTIFICIAL - PROYECTO ML\n",
    "#### Semestre 2023-2 - Grupo 4BV2\n",
    "--- \n",
    "##### Autores:\n",
    "- **Valdés Luis Eliot Fabián**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la librerias necesarias\n",
    "from ucimlrepo import fetch_ucirepo  # libreria de repositorios de datasets\n",
    "import pandas as pd  # libreria para el manejo de dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# PASO 1: Carga de datos\n",
    "Sección inicial para obtener el dataset, describirlo de manera general usando estadisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos los datasets a utilizar\n",
    "datasets = {\n",
    "    \"Eliot\": 53, # Iris\n",
    "    \"Ethel\": 17, # Breast Cancer\n",
    "    \"Leo\": 878, # Cirrhosis\n",
    "    \"Adair\": 109, # Wine\n",
    "}\n",
    "dataset = fetch_ucirepo(id=datasets[\"Ethel\"]) # cargamos el dataset que deseemos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos un pequeño resumen de lo que trata el dataset \n",
    "dataset.metadata.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los datos\n",
    "X = dataset.data.features \n",
    "y = dataset.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un dataframe con los datos\n",
    "df = pd.DataFrame(X, columns=dataset.data.feature_names)\n",
    "# agregamos la columna target\n",
    "df['target'] = y\n",
    "\n",
    "# mostramos la cantidad de registros\n",
    "print(f'Cantidad de registros: {len(df)}')\n",
    "\n",
    "# imprimimos los primeros 5 registros\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los tipos de datos de cada columna\n",
    "print(f\"Tipos de datos (Metodo pandas):\\n{df.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los tipos de datos usando los metodos de la librearia de repositorios\n",
    "dataframe_tipos = pd.DataFrame({'Variable Name': dataset.variables['name'], 'Type': dataset.variables['type']})\n",
    "print(f\"Tipos de datos (Metodo ucimlrepo):\\n{dataframe_tipos}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por cada columna hacer lo siguiente \n",
    "# si el tipo de dato es numerico, mostrar la media, mediana, desviacion estandar, minimo y maximo\n",
    "# si el tipo de dato es categorico, mostrar la cantidad de valores unicos\n",
    "\n",
    "# limitamos la cantidad de columnas a 10\n",
    "for col in df.columns[:10]:\n",
    "    if df[col].dtype == 'float64' or df[col].dtype == 'int64':\n",
    "        print(f\"=> '{col}':\")\n",
    "        print(f\"\\t-Media: {df[col].mean()}\")\n",
    "        print(f\"\\t-Mediana: {df[col].median()}\")\n",
    "        print(f\"\\t-Desviacion estandar: {df[col].std()}\")\n",
    "        print(f\"\\t-Minimo: {df[col].min()}\")\n",
    "        print(f\"\\t-Maximo: {df[col].max()}\")\n",
    "    else:\n",
    "        print(f\"=> '{col}':\")\n",
    "        print(f\"\\t-Valores unicos: {df[col].unique()}\")\n",
    "        print(f\"\\t-Cantidad de valores unicos: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# PASO 2: PREPROCESAMIENTO DE DATOS\n",
    "\n",
    "Sección para preprocesar los datos, aquí separamos el dataset en los vectores de entrada X & de salida Y(las clases que son definidas con tipo de dato categorico)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para separar cualquier dataframe en dos vectores (entrada y salida)\n",
    "def separate_dataframe(df):\n",
    "    vector_x = df.drop(df.columns[-1], axis=1) # obtencion de variables de entrada\n",
    "    vector_y = df[df.columns[-1]] # obtencion de variable de salida (clase/target)\n",
    "    return vector_x, vector_y\n",
    "\n",
    "# separamos el dataframe en dos vectores, uno con las variables independientes y otro con la variable dependiente\n",
    "vector_x, vector_y = separate_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los primeros 5 registros del vector de entrada X\n",
    "vector_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los primeros 5 registros del vector de salida Y\n",
    "vector_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una funcion que recibe como parametros el vector de entrada X y el vector de salida Y\n",
    "def describe_categories(vector_x, vector_y):\n",
    "    # agrupamos el vector de entrada X de acuerdo al vector de salida Y\n",
    "    grouped = vector_x.groupby(vector_y)\n",
    "    for name, group in grouped:\n",
    "        print(\"\\n\",(\"=\"*50))\n",
    "        print(f'Clase: {name}')\n",
    "        print(\"=\"*50)\n",
    "        # por cada grupo, accedemos a cada una de las columnas\n",
    "        for col in group.columns:\n",
    "            # validamos que el tipo de dato de la columna sea numerico y de ser el caso mosstramos las estadisticas\n",
    "            if group[col].dtype == 'float64' or group[col].dtype == 'int64':\n",
    "                print(f\"=> '{col}':\")\n",
    "                print(f\"\\t-Media: {group[col].mean()}\")\n",
    "                print(f\"\\t-Mediana: {group[col].median()}\")\n",
    "                print(f\"\\t-Desviacion estandar: {group[col].std()}\")\n",
    "                print(f\"\\t-Minimo: {group[col].min()}\")\n",
    "                print(f\"\\t-Maximo: {group[col].max()}\")\n",
    "            else:\n",
    "                print(f\"=> '{col}':\")\n",
    "                print(f\"\\tValores unicos: {group[col].unique()}\")\n",
    "                print(f\"\\tCantidad de valores unicos: {group[col].nunique()}\")            \n",
    "        \n",
    "\n",
    "describe_categories(vector_x, vector_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# PASO 3: LIMPIEZA DE DATASET\n",
    "Sección para limpiar el dataset para eliminar valores que no aportan al modelo, como los valores nulos o NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# juntamos los vectores de entrada y salida en un solo dataframe\n",
    "df = vector_x.join(vector_y)\n",
    "# mostramos la cantidad de registros\n",
    "print(f'Cantidad de registros (Before cleaning): {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos los registros que tengan valores nulos en alguna de las columnas\n",
    "df = df.dropna()\n",
    "# mostramos la cantidad de registros\n",
    "print(f'Cantidad de registros: (After cleaning) {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de nuevo separamos el dataframe en dos vectores, uno con las variables independientes y otro con la variable dependiente\n",
    "vector_x, vector_y = separate_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conservamos unicamente las columnas con tipo de dato Inter o Continuous\n",
    "vector_x = vector_x.select_dtypes(include=['int64', 'float64'])\n",
    "# mostramos las primeras 5 filas del vector de entrada\n",
    "vector_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Otra forma de limpiar el vector de entrada. Usa el dataset original y los tipos de datos del dataset pero no es tan eficiente como el metodo anterior ya que alginas columnas con tipo de dato numerico no son numericas en el dataset')\n",
    "\"\"\"\n",
    "for col_name in vector_x.columns:\n",
    "    # verificar el tipo de dato de cada columna usando el dataset\n",
    "    if col_name in dataset.variables['name'].values:\n",
    "        # obtener el tipo de dato de la columna en el dataset\n",
    "        col_type = dataset.variables['type'][dataset.variables['name'] == col_name].values[0]\n",
    "        # eliminamos del vector_x aquellas columnas que no son numericas\n",
    "        if col_type != 'Integer' and col_type != 'Continuous':\n",
    "            vector_x = vector_x.drop(col_name, axis=1)\n",
    "        else:        \n",
    "            # parseamos las columnas a tipo de dato float64\n",
    "            vector_x[col_name] = vector_x[col_name].astype('float64')                    \n",
    "\n",
    "# mostramos las primeras 5 filas del vector de entrada\n",
    "vector_x.head()                    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# PASO 4: IMPLEMENTACIÓN DE MODELOS DE ML Y EVALUACIÓN CON DIFERENTES MÉTRICAS\n",
    "Sección para implementar los modelos Minima Distancia y KNN (K=1) para evaluar con los metodos: train-test split, k-fold cross validation y bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - DESARROLLO DE MODELO MINIMA DISTANCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_centroides(X, y):\n",
    "    clases = set(y)\n",
    "    centroides = {c: [0] * len(X[0]) for c in clases}\n",
    "    contador = {c: 0 for c in clases}\n",
    "\n",
    "    for xi, yi in zip(X, y):\n",
    "        contador[yi] += 1\n",
    "        for i in range(len(xi)):\n",
    "            centroides[yi][i] += xi[i]\n",
    "\n",
    "    for c in centroides:\n",
    "        centroides[c] = [x / contador[c] for x in centroides[c]]\n",
    "    \n",
    "    return centroides\n",
    "\n",
    "def clasificador_minima_distancia(X, centroides):\n",
    "    predicciones = []\n",
    "    for xi in X:\n",
    "        distancias = {c: sum((xi[j] - centroides[c][j])**2 for j in range(len(xi))) for c in centroides}\n",
    "        predicciones.append(min(distancias, key=distancias.get))\n",
    "    return predicciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DESARROLLO DE MODELO K NEAREST NEIGHBORS (K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificador_knn(X_train, y_train, X_test):\n",
    "    predicciones = []\n",
    "    for xi in X_test:\n",
    "        distancias = [sum((xi[j] - X_train[i][j])**2 for j in range(len(xi))) for i in range(len(X_train))]\n",
    "        min_index = distancias.index(min(distancias))\n",
    "        predicciones.append(y_train[min_index])\n",
    "    return predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MÉTODOS DE VALIDACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - DESARROLLO DE MÉTODO TEST-TRAIN SPLIT (80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento_prueba(X, y, porcentaje_prueba):\n",
    "    indice_corte = int(len(X) * (1 - porcentaje_prueba))\n",
    "    X_train = X[:indice_corte]\n",
    "    y_train = y[:indice_corte]\n",
    "    X_test = X[indice_corte:]\n",
    "    y_test = y[indice_corte:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - DESARROLLO DE MÉTODO K-FOLD CROSS VALIDATION (K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k):\n",
    "    tamaño_fold = len(X) // k\n",
    "    for i in range(k):\n",
    "        inicio = i * tamaño_fold\n",
    "        fin = (i + 1) * tamaño_fold if i != k - 1 else len(X)\n",
    "        X_train = X[:inicio] + X[fin:]\n",
    "        y_train = y[:inicio] + y[fin:]\n",
    "        X_test = X[inicio:fin]\n",
    "        y_test = y[inicio:fin]\n",
    "        yield X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - DESARROLLO DE MÉTODO BOOTSTRAPPING (B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def bootstrap(X, y, n):\n",
    "    for _ in range(n):\n",
    "        indices = [random.randint(0, len(X) - 1) for _ in range(len(X))]\n",
    "        X_train = [X[i] for i in indices]\n",
    "        y_train = [y[i] for i in indices]\n",
    "        X_test = [X[i] for i in range(len(X)) if i not in indices]\n",
    "        y_test = [y[i] for i in range(len(X)) if i not in indices]\n",
    "        yield X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCIONES AUXILIARES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - FUNCION PARA CALCULAR LA RELACIÓN EFIENCIA-ERROR DE UN MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_eficiencia_error(y_real, y_pred):\n",
    "    correctos = sum(1 for real, pred in zip(y_real, y_pred) if real == pred)\n",
    "    eficiencia = correctos / len(y_real)\n",
    "    error = 1 - eficiencia\n",
    "    return eficiencia, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - FUNCION PARA TRANSFORMAR UN DATAFRAME EN UNA LISTA DE LISTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_datos(vector_x, vector_y):\n",
    "    X = vector_x.values.tolist() \n",
    "    # Transformar vector_y a una lista\n",
    "    y = vector_y.values.tolist() \n",
    "    y = [item[0] for item in y] if isinstance(y[0], list) else y\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 4 => CONTENEDOR DE EJECUCIÓN DE MODELO: DISTANCIA MINIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_min_distance_model(X, y):\n",
    "    # ENTRENAMIENTO Y PRUEBA\n",
    "    X_train, y_train, X_test, y_test = entrenamiento_prueba(X, y, porcentaje_prueba=0.2)\n",
    "    # Calcular centroides y hacer predicciones\n",
    "    centroides = calcular_centroides(X_train, y_train)\n",
    "    predicciones = clasificador_minima_distancia(X_test, centroides)\n",
    "    # Calcular eficiencia y error\n",
    "    eficiencia, error = calcular_eficiencia_error(y_test, predicciones)\n",
    "    print(f\"Minima distancia: eficiencia = {eficiencia}, error = {error} - Metodo de entrenamiento y prueba\")\n",
    "    \n",
    "    # K-FOLD CROSS VALIDATION\n",
    "    eficiencias = []\n",
    "    errores = []\n",
    "    k = 5  # Número de folds\n",
    "    for X_train, y_train, X_test, y_test in k_fold_cross_validation(X, y, k):\n",
    "        centroides = calcular_centroides(X_train, y_train)\n",
    "        predicciones = clasificador_minima_distancia(X_test, centroides)\n",
    "        eficiencia, error = calcular_eficiencia_error(y_test, predicciones)\n",
    "        eficiencias.append(eficiencia)\n",
    "        errores.append(error)\n",
    "    # Calcular promedios\n",
    "    eficiencia_promedio = sum(eficiencias) / k\n",
    "    error_promedio = sum(errores) / k\n",
    "    print(f\"Minima distancia: eficiencia = {eficiencia_promedio}, error = {error_promedio} - K-fold cross validation\")\n",
    "    \n",
    "    # BOOTSTRAP\n",
    "    eficiencias = []\n",
    "    errores = []\n",
    "    n_iteraciones = 100  # Número de iteraciones de Bootstrap\n",
    "    for X_train, y_train, X_test, y_test in bootstrap(X, y, n_iteraciones):\n",
    "        centroides = calcular_centroides(X_train, y_train)\n",
    "        predicciones = clasificador_minima_distancia(X_test, centroides)\n",
    "        eficiencia, error = calcular_eficiencia_error(y_test, predicciones)\n",
    "        eficiencias.append(eficiencia)\n",
    "        errores.append(error)\n",
    "    # Calcular promedios\n",
    "    eficiencia_promedio = sum(eficiencias) / n_iteraciones\n",
    "    error_promedio = sum(errores) / n_iteraciones\n",
    "    print(f\"Minima distancia: eficiencia = {eficiencia_promedio}, error = {error_promedio} - Bootstrap\")      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 5 => CONTENDOR DE EJECUCIÓN DE MODELO: KNN(K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn_model(X, y):\n",
    "    # ENTRENAMIENTO Y PRUEBA\n",
    "    # Dividir los datos\n",
    "    X_train, y_train, X_test, y_test = entrenamiento_prueba(X, y, porcentaje_prueba=0.2)\n",
    "    # Hacer predicciones\n",
    "    predicciones = clasificador_knn(X_train, y_train, X_test)\n",
    "    # Calcular eficiencia y error\n",
    "    eficiencia, error = calcular_eficiencia_error(y_test, predicciones)\n",
    "    print(f\"KNN: eficiencia = {eficiencia}, error = {error} - Metodo de entrenamiento y prueba\")\n",
    "    \n",
    "    # K-FOLD CROSS VALIDATION\n",
    "    eficiencias = []\n",
    "    errores = []\n",
    "    k = 5  # Número de folds\n",
    "    for X_train, y_train, X_test, y_test in k_fold_cross_validation(X, y, k):\n",
    "        predicciones = clasificador_knn(X_train, y_train, X_test)\n",
    "        eficiencia, error = calcular_eficiencia_error(y_test, predicciones)\n",
    "        eficiencias.append(eficiencia)\n",
    "        errores.append(error)\n",
    "    # Calcular promedios\n",
    "    eficiencia_promedio = sum(eficiencias) / k\n",
    "    error_promedio = sum(errores) / k\n",
    "    print(f\"KNN: eficiencia = {eficiencia_promedio}, error = {error_promedio} - K-fold cross validation\")\n",
    "    \n",
    "    # BOOTSTRAP\n",
    "    eficiencias = []\n",
    "    errores = []\n",
    "    n_iteraciones = 10  # Número de iteraciones de Bootstrap\n",
    "    for X_train, y_train, X_test, y_test in bootstrap(X, y, n_iteraciones):\n",
    "        predicciones = clasificador_knn(X_train, y_train, X_test)\n",
    "        eficiencia, error = calcular_eficiencia_error(y_test, predicciones)\n",
    "        eficiencias.append(eficiencia)\n",
    "        errores.append(error)\n",
    "    # Calcular promedios\n",
    "    eficiencia_promedio = sum(eficiencias) / n_iteraciones\n",
    "    error_promedio = sum(errores) / n_iteraciones\n",
    "    print(f\"KNN: eficiencia = {eficiencia_promedio}, error = {error_promedio} - Bootstrap\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJECUCIÓN DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformamos los vectores de entrada y salida a listas\n",
    "X, y = transformar_datos(vector_x, vector_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# PASO 4: EJECUCIÓN DE DISTANCIA MINIMA\n",
    "- #### 4.A => DISTANCIA MINIMA - EVALUADO CON: TRAIN-TEST SPLIT\n",
    "- #### 4.B => DISTANCIA MINIMA - EVALUADO CON: K FOLD CROSS VALIDATION\n",
    "- #### 4.C => DISTANCIA MINIMA - EVALUADO CON: BOOTSTRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos el modelo de distancia minima y validamos internamente con cada metodo\n",
    "run_min_distance_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# PASO 5: EJECUCIÓN DE KNN(K=1)\n",
    "- #### 5.A => KNN - EVALUADO CON: TRAIN-TEST SPLIT\n",
    "- #### 5.B => KNN - EVALUADO CON: K FOLD CROSS VALIDATION\n",
    "- #### 5.C => KNN - EVALUADO CON: BOOTSTRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos el modelo de KNN y validamos internamente con cada metodo\n",
    "run_knn_model(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
